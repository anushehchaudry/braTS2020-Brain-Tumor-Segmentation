{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIj3cvMGV1Cd",
        "outputId": "c4892ab0-6eb4-4d7b-f257-14129fff86c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment2/'\n",
        "FOLDERNAME = 'cs231n_final/braTS-2020-master/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = '/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020'\n",
        "TRAIN_PATH = f'{DATASET_PATH}/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
        "VALIDATION_PATH = f'{DATASET_PATH}/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
        "\n",
        "# clean ds\n",
        "CLEAN_DS_PATH = '/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020/3c_all_split'\n",
        "\n",
        "# train\n",
        "CLEAN_TRAIN_PATH = f'{CLEAN_DS_PATH}/train'\n",
        "CLEAN_TRAIN_IMG_PATH = f'{CLEAN_TRAIN_PATH}/images'\n",
        "CLEAN_TRAIN_MSK_PATH = f'{CLEAN_TRAIN_PATH}/masks'\n",
        "\n",
        "# val\n",
        "CLEAN_VAL_PATH = f'{CLEAN_DS_PATH}/val'\n",
        "CLEAN_VAL_IMG_PATH = f'{CLEAN_VAL_PATH}/images'\n",
        "CLEAN_VAL_MSK_PATH = f'{CLEAN_VAL_PATH}/masks'\n"
      ],
      "metadata": {
        "id": "5-ld7ACsV_rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "L7jTq78rYpPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogger:\n",
        "\n",
        "    def __init__(self, debug=True):\n",
        "        self.debug = debug\n",
        "\n",
        "    def enable_debug(self):\n",
        "        self.debug = True\n",
        "\n",
        "    def disable_debug(self):\n",
        "        self.debug = False\n",
        "\n",
        "    def log(self, message, condition=True):\n",
        "        if self.debug and condition:\n",
        "            print(message)\n",
        "\n",
        "\n",
        "logger = SimpleLogger(debug=True)\n",
        "\n",
        "\n",
        "def to_categorical(y, n_classes):\n",
        "    return np.eye(n_classes, dtype=\"uint8\")[y]\n",
        "\n",
        "\n",
        "class BraTSDataset(Dataset):\n",
        "\n",
        "    def log(self, message):\n",
        "        logger.log(message, condition=self.debug)\n",
        "\n",
        "    def __init__(self, images_path, masks_path, transform=None, one_hot_target=True, debug=True, original_mask=False):\n",
        "        self.images = sorted(glob(f\"{images_path}/*.npy\"))\n",
        "        self.masks = sorted(glob(f\"{masks_path}/*.npy\"))\n",
        "        self.transform = transform\n",
        "        self.one_hot_target = one_hot_target\n",
        "        self.debug = debug\n",
        "        self.original_masks = original_mask\n",
        "        self.log(f\"images: {len(self.images)}, masks: {len(self.masks)} \")\n",
        "        assert len(self.images) == len(self.masks), \"images and masks lengths are not the same!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # if torch.is_tensor(idx):\n",
        "        #     idx = idx.tolist()\n",
        "\n",
        "        image = np.load(self.images[idx])\n",
        "        mask = np.load(self.masks[idx])\n",
        "        # resizing image and mask, experimental\n",
        "        image = image[::2, ::2, ::2]\n",
        "        if not self.original_masks:\n",
        "            mask = mask[::2, ::2, ::2]\n",
        "        if self.one_hot_target:\n",
        "            mask = to_categorical(mask, 4)\n",
        "            mask = mask[::, ::, ::, 1::]  # discard background\n",
        "\n",
        "        image = torch.from_numpy(image).float()  # .double()\n",
        "        mask = torch.from_numpy(mask)  # .float() #.long()\n",
        "\n",
        "        return image.permute((3, 0, 1, 2)), mask.permute((3, 0, 1, 2))\n",
        "\n",
        "\n",
        "def get_train_ds():\n",
        "    return BraTSDataset(CLEAN_TRAIN_IMG_PATH, CLEAN_TRAIN_MSK_PATH)\n",
        "\n",
        "\n",
        "def get_val_ds(full_masks=False):\n",
        "    return BraTSDataset(CLEAN_VAL_IMG_PATH, CLEAN_VAL_MSK_PATH, original_mask=full_masks)\n",
        "\n",
        "\n",
        "def get_dl(dataset, batch_size=32, pm=True, nw=4):\n",
        "    return DataLoader(dataset, batch_size, shuffle=True, pin_memory=pm, num_workers=nw, )\n"
      ],
      "metadata": {
        "id": "gnaIGN2n_JdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "def to_categorical(y, n_classes):\n",
        "    return np.eye(n_classes, dtype=\"uint8\")[y]\n",
        "\n",
        "\n",
        "def nib_load(path):\n",
        "    return nib.load(path).get_data()"
      ],
      "metadata": {
        "id": "gFl631gEY9nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_flair=nib.load(TRAIN_PATH + '/BraTS20_Training_355/BraTS20_Training_355_flair.nii').get_fdata()\n",
        "print(test_image_flair.shape)\n",
        "print(test_image_flair.min())\n",
        "print(test_image_flair.max())\n",
        "# #Scalers are applied to 1D so let us reshape and then reshape back to original shape.\n",
        "test_image_flair=scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)\n",
        "#\n",
        "#\n",
        "test_image_t1=nib.load(TRAIN_PATH + '/BraTS20_Training_355/BraTS20_Training_355_t1.nii').get_fdata()\n",
        "test_image_t1=scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)\n",
        "print(test_image_t1.shape)\n",
        "print(test_image_t1.min())\n",
        "print(test_image_t1.max())\n",
        "test_image_t1ce=nib.load(TRAIN_PATH + '/BraTS20_Training_355/BraTS20_Training_355_t1ce.nii').get_fdata()\n",
        "test_image_t1ce=scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)\n",
        "print(test_image_t1ce.shape)\n",
        "print(test_image_t1ce.min())\n",
        "print(test_image_t1ce.max())\n",
        "test_image_t2=nib.load(TRAIN_PATH + '/BraTS20_Training_355/BraTS20_Training_355_t2.nii').get_fdata()\n",
        "test_image_t2=scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)\n",
        "print(test_image_t2.shape)\n",
        "print(test_image_t2.min())\n",
        "print(test_image_t2.max())\n",
        "#\n",
        "test_mask=nib.load(TRAIN_PATH + '/BraTS20_Training_355/BraTS20_Training_355_seg.nii').get_fdata()\n",
        "test_mask=test_mask.astype(np.uint8)\n",
        "print(test_mask.shape)\n",
        "print(test_mask.min())\n",
        "print(test_mask.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zrIm1UuZWvu",
        "outputId": "796390ec-2fe6-4cd6-80e2-fa9965eef425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 240, 155)\n",
            "0.0\n",
            "1854.603271484375\n",
            "(240, 240, 155)\n",
            "0.0\n",
            "1.0\n",
            "(240, 240, 155)\n",
            "0.0\n",
            "1.0\n",
            "(240, 240, 155)\n",
            "0.0\n",
            "1.0\n",
            "(240, 240, 155)\n",
            "0\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(test_mask))\n",
        "test_mask[test_mask==4] = 3\n",
        "print(np.unique(test_mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaqWr-2cZyP0",
        "outputId": "73628974-bac4-4fc6-ff67-2d955106edbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 4]\n",
            "[0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "w50xp3BbaBwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t2_list = sorted(glob.glob(f'{TRAIN_PATH}/*/*t2.nii'))  # 369 training instances\n",
        "t1ce_list = sorted(glob.glob(f'{TRAIN_PATH}/*/*t1ce.nii'))\n",
        "flair_list = sorted(glob.glob(f'{TRAIN_PATH}/*/*flair.nii'))\n",
        "mask_list = sorted(glob.glob(f'{TRAIN_PATH}/*/*seg.nii'))"
      ],
      "metadata": {
        "id": "hrSWc9vSZ3Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_t2_list = sorted(glob.glob(f'{VALIDATION_PATH}/*/*t2.nii'))  # 125 validation data instances\n",
        "v_t1ce_list = sorted(glob.glob(f'{VALIDATION_PATH}/*/*t1ce.nii'))\n",
        "v_flair_list = sorted(glob.glob(f'{VALIDATION_PATH}/*/*flair.nii'))\n",
        "v_mask_list = sorted(glob.glob(f'{VALIDATION_PATH}/*/*seg.nii'))"
      ],
      "metadata": {
        "id": "urTKkiL5aSnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(v_flair_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_-tHycoqapU",
        "outputId": "5a4aedfa-4555-45f9-cf19-a52a8c330496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img in range(len(t2_list)):\n",
        "    print(\"image index at:\", img)\n",
        "\n",
        "    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n",
        "    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(\n",
        "        temp_image_t2.shape)\n",
        "\n",
        "    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n",
        "    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(\n",
        "        temp_image_t1ce.shape)\n",
        "\n",
        "    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n",
        "    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(\n",
        "        temp_image_flair.shape)\n",
        "\n",
        "    temp_mask = nib.load(mask_list[img]).get_fdata()\n",
        "    temp_mask = temp_mask.astype(np.uint8)\n",
        "    temp_mask[temp_mask == 4] = 3\n",
        "\n",
        "    temp_combined_images = np.stack([temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3)\n",
        "\n",
        "    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
        "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
        "\n",
        "    val, counts = np.unique(temp_mask, return_counts=True)\n",
        "\n",
        "    # temp_mask= to_categorical(temp_mask, num_classes=4)\n",
        "    np.save(f'{TRAIN_PATH}/3c_all/images/image_' + str(img) + '.npy', temp_combined_images)\n",
        "    np.save(f'{TRAIN_PATH}/3c_all/masks/mask_' + str(img) + '.npy', temp_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wM4QByTNadDs",
        "outputId": "2e65ec3b-9433-4c1b-8f9e-8980ae254184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image index at: 0\n",
            "image index at: 1\n",
            "image index at: 2\n",
            "image index at: 3\n",
            "image index at: 4\n",
            "image index at: 5\n",
            "image index at: 6\n",
            "image index at: 7\n",
            "image index at: 8\n",
            "image index at: 9\n",
            "image index at: 10\n",
            "image index at: 11\n",
            "image index at: 12\n",
            "image index at: 13\n",
            "image index at: 14\n",
            "image index at: 15\n",
            "image index at: 16\n",
            "image index at: 17\n",
            "image index at: 18\n",
            "image index at: 19\n",
            "image index at: 20\n",
            "image index at: 21\n",
            "image index at: 22\n",
            "image index at: 23\n",
            "image index at: 24\n",
            "image index at: 25\n",
            "image index at: 26\n",
            "image index at: 27\n",
            "image index at: 28\n",
            "image index at: 29\n",
            "image index at: 30\n",
            "image index at: 31\n",
            "image index at: 32\n",
            "image index at: 33\n",
            "image index at: 34\n",
            "image index at: 35\n",
            "image index at: 36\n",
            "image index at: 37\n",
            "image index at: 38\n",
            "image index at: 39\n",
            "image index at: 40\n",
            "image index at: 41\n",
            "image index at: 42\n",
            "image index at: 43\n",
            "image index at: 44\n",
            "image index at: 45\n",
            "image index at: 46\n",
            "image index at: 47\n",
            "image index at: 48\n",
            "image index at: 49\n",
            "image index at: 50\n",
            "image index at: 51\n",
            "image index at: 52\n",
            "image index at: 53\n",
            "image index at: 54\n",
            "image index at: 55\n",
            "image index at: 56\n",
            "image index at: 57\n",
            "image index at: 58\n",
            "image index at: 59\n",
            "image index at: 60\n",
            "image index at: 61\n",
            "image index at: 62\n",
            "image index at: 63\n",
            "image index at: 64\n",
            "image index at: 65\n",
            "image index at: 66\n",
            "image index at: 67\n",
            "image index at: 68\n",
            "image index at: 69\n",
            "image index at: 70\n",
            "image index at: 71\n",
            "image index at: 72\n",
            "image index at: 73\n",
            "image index at: 74\n",
            "image index at: 75\n",
            "image index at: 76\n",
            "image index at: 77\n",
            "image index at: 78\n",
            "image index at: 79\n",
            "image index at: 80\n",
            "image index at: 81\n",
            "image index at: 82\n",
            "image index at: 83\n",
            "image index at: 84\n",
            "image index at: 85\n",
            "image index at: 86\n",
            "image index at: 87\n",
            "image index at: 88\n",
            "image index at: 89\n",
            "image index at: 90\n",
            "image index at: 91\n",
            "image index at: 92\n",
            "image index at: 93\n",
            "image index at: 94\n",
            "image index at: 95\n",
            "image index at: 96\n",
            "image index at: 97\n",
            "image index at: 98\n",
            "image index at: 99\n",
            "image index at: 100\n",
            "image index at: 101\n",
            "image index at: 102\n",
            "image index at: 103\n",
            "image index at: 104\n",
            "image index at: 105\n",
            "image index at: 106\n",
            "image index at: 107\n",
            "image index at: 108\n",
            "image index at: 109\n",
            "image index at: 110\n",
            "image index at: 111\n",
            "image index at: 112\n",
            "image index at: 113\n",
            "image index at: 114\n",
            "image index at: 115\n",
            "image index at: 116\n",
            "image index at: 117\n",
            "image index at: 118\n",
            "image index at: 119\n",
            "image index at: 120\n",
            "image index at: 121\n",
            "image index at: 122\n",
            "image index at: 123\n",
            "image index at: 124\n",
            "image index at: 125\n",
            "image index at: 126\n",
            "image index at: 127\n",
            "image index at: 128\n",
            "image index at: 129\n",
            "image index at: 130\n",
            "image index at: 131\n",
            "image index at: 132\n",
            "image index at: 133\n",
            "image index at: 134\n",
            "image index at: 135\n",
            "image index at: 136\n",
            "image index at: 137\n",
            "image index at: 138\n",
            "image index at: 139\n",
            "image index at: 140\n",
            "image index at: 141\n",
            "image index at: 142\n",
            "image index at: 143\n",
            "image index at: 144\n",
            "image index at: 145\n",
            "image index at: 146\n",
            "image index at: 147\n",
            "image index at: 148\n",
            "image index at: 149\n",
            "image index at: 150\n",
            "image index at: 151\n",
            "image index at: 152\n",
            "image index at: 153\n",
            "image index at: 154\n",
            "image index at: 155\n",
            "image index at: 156\n",
            "image index at: 157\n",
            "image index at: 158\n",
            "image index at: 159\n",
            "image index at: 160\n",
            "image index at: 161\n",
            "image index at: 162\n",
            "image index at: 163\n",
            "image index at: 164\n",
            "image index at: 165\n",
            "image index at: 166\n",
            "image index at: 167\n",
            "image index at: 168\n",
            "image index at: 169\n",
            "image index at: 170\n",
            "image index at: 171\n",
            "image index at: 172\n",
            "image index at: 173\n",
            "image index at: 174\n",
            "image index at: 175\n",
            "image index at: 176\n",
            "image index at: 177\n",
            "image index at: 178\n",
            "image index at: 179\n",
            "image index at: 180\n",
            "image index at: 181\n",
            "image index at: 182\n",
            "image index at: 183\n",
            "image index at: 184\n",
            "image index at: 185\n",
            "image index at: 186\n",
            "image index at: 187\n",
            "image index at: 188\n",
            "image index at: 189\n",
            "image index at: 190\n",
            "image index at: 191\n",
            "image index at: 192\n",
            "image index at: 193\n",
            "image index at: 194\n",
            "image index at: 195\n",
            "image index at: 196\n",
            "image index at: 197\n",
            "image index at: 198\n",
            "image index at: 199\n",
            "image index at: 200\n",
            "image index at: 201\n",
            "image index at: 202\n",
            "image index at: 203\n",
            "image index at: 204\n",
            "image index at: 205\n",
            "image index at: 206\n",
            "image index at: 207\n",
            "image index at: 208\n",
            "image index at: 209\n",
            "image index at: 210\n",
            "image index at: 211\n",
            "image index at: 212\n",
            "image index at: 213\n",
            "image index at: 214\n",
            "image index at: 215\n",
            "image index at: 216\n",
            "image index at: 217\n",
            "image index at: 218\n",
            "image index at: 219\n",
            "image index at: 220\n",
            "image index at: 221\n",
            "image index at: 222\n",
            "image index at: 223\n",
            "image index at: 224\n",
            "image index at: 225\n",
            "image index at: 226\n",
            "image index at: 227\n",
            "image index at: 228\n",
            "image index at: 229\n",
            "image index at: 230\n",
            "image index at: 231\n",
            "image index at: 232\n",
            "image index at: 233\n",
            "image index at: 234\n",
            "image index at: 235\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c85bab4f1641>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# temp_mask= to_categorical(temp_mask, num_classes=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{TRAIN_PATH}/3c_all/images/image_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_combined_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{TRAIN_PATH}/3c_all/masks/mask_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders[full]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-afU9h34q6aS",
        "outputId": "6bfeffdd-7fc3-4324-b7d2-a3f2fb3ac9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders[full]\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from split-folders[full]) (4.65.0)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders # the original data's validation cannot be used as test, due to a lack of masks\n",
        "\n",
        "clean_data_folder = '/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/3c_all'\n",
        "target_folder = '/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020/3c_all_split'\n",
        "splitfolders.ratio(clean_data_folder, output=target_folder, seed=42, ratio=(.6, .4), group_prefix=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "cR2xZbzziDAH",
        "outputId": "368c63af-7c7e-4c42-b513-94016b132684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 4 files [00:07,  1.91s/ files]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-00cc2640334e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclean_data_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/3c_all'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shareddrives/cs231n_final/cs231n_baseline/BraTS2020/3c_all_split'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msplitfolders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(input, output, seed, ratio, group_prefix, move)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         split_class_dir_ratio(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/splitfolders/split.py\u001b[0m in \u001b[0;36msplit_class_dir_ratio\u001b[0;34m(class_dir, output, ratio, seed, prog_bar, group_prefix, move)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_train_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_val_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mcopy_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mcopy_files\u001b[0;34m(files_type, class_dir, output, prog_bar, move)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0mcopy_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mcopy_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0m_fastcopy_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPYFILE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# must load the model to the device and also prep the loss function with model parameters before using this function\n",
        "# also do not forget to initialize the optimizer with the desired learning rate\n",
        "def train(model, epochs=1, training_loader=None, loss_fn=None, device=None,\n",
        "          optimizer: torch.optim.Optimizer = None, from_epoch=0):  # from_epoch is for the resume mode\n",
        "    for epoch in range(from_epoch, epochs):\n",
        "        tq_dl = tqdm(training_loader)\n",
        "        for idx, (image, mask) in enumerate(tq_dl):\n",
        "            image, mask = image.to(device), mask.to(device)\n",
        "            # forward pass\n",
        "            out = model(image)\n",
        "            loss = loss_fn(out, mask)\n",
        "            # backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            tq_dl.set_description(f\"At epoch [{epoch + 1}/{epochs}]\")\n",
        "            tq_dl.set_postfix(loss=loss.item())  # acc, ...\n",
        "\n",
        "\n",
        "def check_accuracy(data_loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)  # .unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / (\n",
        "                    (preds + y).sum() + 1e-8\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"Results: {num_correct}/{num_pixels} with accuracy {num_correct / num_pixels * 100:.4f}\"\n",
        "    )\n",
        "    print(f\"Dice score: {dice_score / len(data_loader)}\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# our labels\n",
        "# 1: Necrotic and Non-enhancing tumor core (NCR/NET)\n",
        "# 2: Peritumoral Edema (ED)\n",
        "# 3: GD-enhancing tumor (ET)\n",
        "# common stuff in other papers:\n",
        "# TC -> 1 + 3\n",
        "# ET -> 3\n",
        "# WT -> TC + ED: all\n",
        "def check_accuracy_v2(data_loader, model, device=\"cuda\"):\n",
        "    num_correct = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_pixels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    dice_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)  # .unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds >= 0.5).float()\n",
        "            TC_pred = (preds[:, 0] + preds[:, 2] >= 1).float()\n",
        "            TC_real = (y[:, 0] + y[:, 2] >= 1).float()\n",
        "            ET_pred = preds[:, 2]\n",
        "            ET_real = y[:, 2]\n",
        "            WT_pred = (preds[:, 0] + preds[:, 1] + preds[:, 2] >= 1).float()\n",
        "            WT_real = (y[:, 0] + y[:, 1] + y[:, 2] >= 1).float()\n",
        "\n",
        "            num_correct['TC'] += (TC_pred == TC_real).sum()\n",
        "            num_pixels['TC'] += torch.numel(TC_pred)\n",
        "            dice_score['TC'] += (2 * (TC_pred * TC_real).sum()) / (\n",
        "                    (TC_pred + TC_real).sum() + 1e-8)\n",
        "\n",
        "            num_correct['ET'] += (ET_pred == ET_real).sum()\n",
        "            num_pixels['ET'] += torch.numel(ET_pred)\n",
        "            dice_score['ET'] += (2 * (ET_pred * ET_real).sum()) / (\n",
        "                    (ET_pred + ET_real).sum() + 1e-8)\n",
        "\n",
        "            num_correct['WT'] += (WT_pred == WT_real).sum()\n",
        "            num_pixels['WT'] += torch.numel(WT_pred)\n",
        "            dice_score['WT'] += (2 * (WT_pred * WT_real).sum()) / (\n",
        "                    (WT_pred + WT_real).sum() + 1e-8)\n",
        "\n",
        "    # print(\n",
        "    #     f\"Results (TC,ET,WT): ({num_correct['TC']}/{num_pixels['TC']}) with accuracy {num_correct / num_pixels * 100:.4f}\"\n",
        "    # )\n",
        "\n",
        "    print(\n",
        "        f\" Accuracy (TC,ET,WT): \\n --> {num_correct['TC'] / num_pixels['TC'] * 100:.4f} , {num_correct['ET'] / num_pixels['ET'] * 100:.4f}, {num_correct['WT'] / num_pixels['WT'] * 100:.4f}\")\n",
        "    print(\n",
        "        f\"Dice Score (TC,ET,WT): \\n {dice_score['TC'] / len(data_loader)} , {dice_score['ET'] / len(data_loader)}, {dice_score['WT'] / len(data_loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def check_accuracy_v3(data_loader, model, device=\"cuda\"):  # requires full masks\n",
        "    num_correct = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_pixels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    dice_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y1 in data_loader:\n",
        "            y = y1[:, :, ::2, ::2, ::2]  # get the halved mask\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)  # .unsqueeze(1)\n",
        "            y, y1 = y1, y\n",
        "            original_preds = torch.sigmoid(model(x))\n",
        "\n",
        "            preds = simple_trilinear_interpolation(original_preds)  # here or next?\n",
        "\n",
        "            preds = (preds >= 0.5).float()\n",
        "            TC_pred = (preds[:, 0] + preds[:, 2] >= 1).float()\n",
        "            TC_real = (y[:, 0] + y[:, 2] >= 1).float()\n",
        "            ET_pred = preds[:, 2]\n",
        "            ET_real = y[:, 2]\n",
        "            WT_pred = (preds[:, 0] + preds[:, 1] + preds[:, 2] >= 1).float()\n",
        "            WT_real = (y[:, 0] + y[:, 1] + y[:, 2] >= 1).float()\n",
        "\n",
        "            num_correct['TC'] += (TC_pred == TC_real).sum()\n",
        "            num_pixels['TC'] += torch.numel(TC_pred)\n",
        "            dice_score['TC'] += (2 * (TC_pred * TC_real).sum()) / (\n",
        "                    (TC_pred + TC_real).sum() + 1e-8)\n",
        "\n",
        "            num_correct['ET'] += (ET_pred == ET_real).sum()\n",
        "            num_pixels['ET'] += torch.numel(ET_pred)\n",
        "            dice_score['ET'] += (2 * (ET_pred * ET_real).sum()) / (\n",
        "                    (ET_pred + ET_real).sum() + 1e-8)\n",
        "\n",
        "            num_correct['WT'] += (WT_pred == WT_real).sum()\n",
        "            num_pixels['WT'] += torch.numel(WT_pred)\n",
        "            dice_score['WT'] += (2 * (WT_pred * WT_real).sum()) / (\n",
        "                    (WT_pred + WT_real).sum() + 1e-8)\n",
        "\n",
        "    # print(\n",
        "    #     f\"Results (TC,ET,WT): ({num_correct['TC']}/{num_pixels['TC']}) with accuracy {num_correct / num_pixels * 100:.4f}\"\n",
        "    # )\n",
        "\n",
        "    print(\n",
        "        f\" Accuracy (TC,ET,WT): \\n --> {num_correct['TC'] / num_pixels['TC'] * 100:.4f} , {num_correct['ET'] / num_pixels['ET'] * 100:.4f}, {num_correct['WT'] / num_pixels['WT'] * 100:.4f}\")\n",
        "    print(\n",
        "        f\"Dice Score (TC,ET,WT): \\n {dice_score['TC'] / len(data_loader)} , {dice_score['ET'] / len(data_loader)}, {dice_score['WT'] / len(data_loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def get_all_metrics(data_loader, model, device=\"cuda\"):  # requires full masks\n",
        "    num_correct = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_true_predicts = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_true_labels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_pixels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    dice_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    iou_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "\n",
        "    EPS = 1e-16\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y1 in data_loader:\n",
        "            y = y1[:, :, ::2, ::2, ::2]  # get the halved mask\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)  # .unsqueeze(1)\n",
        "            y, y1 = y1, y\n",
        "            original_preds = torch.sigmoid(model(x))\n",
        "\n",
        "            preds = simple_trilinear_interpolation(original_preds)  # here or next?\n",
        "\n",
        "            preds = (preds >= 0.5).float()\n",
        "            TC_pred = (preds[:, 0] + preds[:, 2] > 0.9).float()\n",
        "            TC_real = (y[:, 0] + y[:, 2] > 0.9).float()\n",
        "            ET_pred = preds[:, 2]\n",
        "            ET_real = y[:, 2]\n",
        "            WT_pred = (preds[:, 0] + preds[:, 1] + preds[:, 2] > 0.9).float()\n",
        "            WT_real = (y[:, 0] + y[:, 1] + y[:, 2] > 0.9).float()\n",
        "\n",
        "            num_correct['TC'] += (TC_pred == TC_real).sum()\n",
        "            num_pixels['TC'] += torch.numel(TC_pred)\n",
        "            num_true_predicts['TC'] += (TC_pred > .9).sum()  # those that are 1\n",
        "            num_true_labels['TC'] += (TC_real > .9).sum()\n",
        "            dice_score['TC'] += (2 * (TC_pred * TC_real).sum()) / ((TC_pred + TC_real).sum() + EPS)\n",
        "            iou_score['TC'] += (TC_pred * TC_real).sum() / ((TC_pred + TC_real).sum() - (TC_pred * TC_real).sum() + EPS)\n",
        "\n",
        "            num_correct['ET'] += (ET_pred == ET_real).sum()\n",
        "            num_pixels['ET'] += torch.numel(ET_pred)\n",
        "            num_true_predicts['ET'] += (ET_pred > .9).sum()\n",
        "            num_true_labels['ET'] += (ET_real > .9).sum()\n",
        "            dice_score['ET'] += (2 * (ET_pred * ET_real).sum()) / ((ET_pred + ET_real).sum() + EPS)\n",
        "            iou_score['ET'] += (ET_pred * ET_real).sum() / ((ET_pred + ET_real).sum() - (ET_pred * ET_real).sum() + EPS)\n",
        "\n",
        "            num_correct['WT'] += (WT_pred == WT_real).sum()\n",
        "            num_pixels['WT'] += torch.numel(WT_pred)\n",
        "            num_true_predicts['WT'] += (WT_pred > .9).sum()\n",
        "            num_true_labels['WT'] += (WT_real > .9).sum()\n",
        "            dice_score['WT'] += (2 * (WT_pred * WT_real).sum()) / ((WT_pred + WT_real).sum() + EPS)\n",
        "            iou_score['WT'] += (WT_pred * WT_real).sum() / ((WT_pred + WT_real).sum() - (WT_pred * WT_real).sum() + EPS)\n",
        "\n",
        "    # precision: correct true predicts / all true predicts -> TP / (TP + FP)\n",
        "\n",
        "    precision = {'TC': num_correct['TC'] / num_true_predicts['TC'] * 100,\n",
        "                 'ET': num_correct['ET'] / num_true_predicts['ET'] * 100,\n",
        "                 'WT': num_correct['WT'] / num_true_predicts['WT'] * 100}\n",
        "    # recall: correct true predicts / all true labels -> TP / (TP + FN)\n",
        "\n",
        "    recall = {'TC': num_correct['TC'] / num_true_labels['TC'] * 100,\n",
        "              'ET': num_correct['ET'] / num_true_labels['ET'] * 100,\n",
        "              'WT': num_correct['WT'] / num_true_labels['WT'] * 100}\n",
        "\n",
        "    # f1 is actually the same as Dice Coefficient, or is it?\n",
        "    f1 = {'TC': 2 * precision['TC'] * recall['TC'] / (precision['TC'] + recall['TC']),\n",
        "          'ET': 2 * precision['ET'] * recall['ET'] / (precision['ET'] + recall['ET']),\n",
        "          'WT': 2 * precision['WT'] * recall['WT'] / (precision['WT'] + recall['WT'])}\n",
        "\n",
        "    print(\n",
        "        f\" Accuracy (TC,ET,WT): \\n {num_correct['TC'] / num_pixels['TC'] * 100:.4f} , {num_correct['ET'] / num_pixels['ET'] * 100:.4f}, {num_correct['WT'] / num_pixels['WT'] * 100:.4f}\")\n",
        "    print(\n",
        "        f\"Dice Score (TC,ET,WT): \\n {dice_score['TC'] / len(data_loader)} , {dice_score['ET'] / len(data_loader)}, {dice_score['WT'] / len(data_loader)}\")\n",
        "    print(\n",
        "        f\"IoU Score (TC,ET,WT): \\n {iou_score['TC'] / len(data_loader)} , {iou_score['ET'] / len(data_loader)}, {iou_score['WT'] / len(data_loader)}\")\n",
        "\n",
        "    print(\n",
        "        f\" Precision (TC,ET,WT): \\n --> {precision['TC'] / len(data_loader):.4f} , {precision['ET'] / len(data_loader):.4f}, {precision['WT'] / len(data_loader):.4f}\")\n",
        "    print(\n",
        "        f\" Recall (TC,ET,WT): \\n --> {recall['TC'] / len(data_loader):.4f} , {recall['ET'] / len(data_loader):.4f}, {recall['WT'] / len(data_loader):.4f}\")\n",
        "    print(\n",
        "        f\" F1-score (TC,ET,WT): \\n --> {f1['TC'] / len(data_loader):.4f} , {f1['ET'] / len(data_loader):.4f}, {f1['WT'] / len(data_loader):.4f}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return None\n",
        "\n",
        "# This one gives true average on all, and is final\n",
        "def get_all_metrics_2(data_loader, model, device=\"cuda\"):  # requires full masks\n",
        "    num_correct = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_true_predicts = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_true_labels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    num_pixels = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    dice_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "    iou_score = {'TC': 0, 'ET': 0, 'WT': 0}\n",
        "\n",
        "    results = {'TC': {'real': [], 'pred': []}, 'ET': {'real': [], 'pred': []}, 'WT': {'real': [], 'pred': []}}\n",
        "\n",
        "    EPS = 1e-9\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y1 in data_loader:\n",
        "            y = y1[:, :, ::2, ::2, ::2]  # get the halved mask\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)  # .unsqueeze(1)\n",
        "            y, y1 = y1, y\n",
        "            original_preds = torch.sigmoid(model(x))\n",
        "\n",
        "            preds = simple_trilinear_interpolation(original_preds)  # here or next?\n",
        "\n",
        "            # preds = (preds >= 0.5).float() # can't do it here, need the probabilities for auc_roc\n",
        "            for i in range(preds.shape[0]):\n",
        "                # TC_pred = (preds[i, 0] + preds[i, 2] > 0.9).float()\n",
        "                # ET_pred = preds[i, 2]\n",
        "                # WT_pred = (preds[i, 0] + preds[i, 1] + preds[i, 2] > 0.9).float()\n",
        "\n",
        "                TC_real = (y[i, 0] + y[i, 2] > 0.9).float()\n",
        "                ET_real = y[i, 2]\n",
        "                WT_real = (y[i, 0] + y[i, 1] + y[i, 2] > 0.9).float()\n",
        "\n",
        "                TC_pred = (preds[i, 0] + preds[i, 2]).float()\n",
        "                ET_pred = preds[i, 2]\n",
        "                WT_pred = (preds[i, 0] + preds[i, 1] + preds[i, 2]).float()\n",
        "\n",
        "                # for auc_roc we need non-thresholded probabilities\n",
        "                results['TC']['pred'].append(TC_pred.flatten().cpu().numpy())\n",
        "                results['TC']['real'].append(TC_real.flatten().cpu().numpy())\n",
        "                results['ET']['pred'].append(ET_pred.flatten().cpu().numpy())\n",
        "                results['ET']['real'].append(ET_real.flatten().cpu().numpy())\n",
        "                results['WT']['pred'].append(WT_pred.flatten().cpu().numpy())\n",
        "                results['WT']['real'].append(WT_real.flatten().cpu().numpy())\n",
        "\n",
        "                # Thresholding again\n",
        "                TC_pred = (TC_pred >= 0.5).float()\n",
        "                ET_pred = (ET_pred >= 0.5).float()\n",
        "                WT_pred = (WT_pred >= 0.5).float()\n",
        "\n",
        "                num_correct['TC'] += (TC_pred == TC_real).sum()\n",
        "                num_pixels['TC'] += torch.numel(TC_pred)\n",
        "                num_true_predicts['TC'] += (TC_pred > .9).sum()  # those that are 1\n",
        "                num_true_labels['TC'] += (TC_real > .9).sum()\n",
        "                dice_score['TC'] += (2 * (TC_pred * TC_real).sum()) / ((TC_pred + TC_real).sum() + EPS)\n",
        "                iou_score['TC'] += (TC_pred * TC_real).sum() / (\n",
        "                        (TC_pred + TC_real).sum() - (TC_pred * TC_real).sum() + EPS)\n",
        "\n",
        "                num_correct['ET'] += (ET_pred == ET_real).sum()\n",
        "                num_pixels['ET'] += torch.numel(ET_pred)\n",
        "                num_true_predicts['ET'] += (ET_pred > .9).sum()\n",
        "                num_true_labels['ET'] += (ET_real > .9).sum()\n",
        "                dice_score['ET'] += (2 * (ET_pred * ET_real).sum()) / ((ET_pred + ET_real).sum() + EPS)\n",
        "                iou_score['ET'] += (ET_pred * ET_real).sum() / (\n",
        "                        (ET_pred + ET_real).sum() - (ET_pred * ET_real).sum() + EPS)\n",
        "\n",
        "                num_correct['WT'] += (WT_pred == WT_real).sum()\n",
        "                num_pixels['WT'] += torch.numel(WT_pred)\n",
        "                num_true_predicts['WT'] += (WT_pred > .9).sum()\n",
        "                num_true_labels['WT'] += (WT_real > .9).sum()\n",
        "                dice_score['WT'] += (2 * (WT_pred * WT_real).sum()) / ((WT_pred + WT_real).sum() + EPS)\n",
        "                iou_score['WT'] += (WT_pred * WT_real).sum() / (\n",
        "                        (WT_pred + WT_real).sum() - (WT_pred * WT_real).sum() + EPS)\n",
        "\n",
        "    # precision: correct true predicts / all true predicts -> TP / (TP + FP)\n",
        "\n",
        "    precision = {'TC': num_correct['TC'] / num_true_predicts['TC'] * 100,\n",
        "                 'ET': num_correct['ET'] / num_true_predicts['ET'] * 100,\n",
        "                 'WT': num_correct['WT'] / num_true_predicts['WT'] * 100}\n",
        "    # recall: correct true predicts / all true labels -> TP / (TP + FN)\n",
        "\n",
        "    recall = {'TC': num_correct['TC'] / num_true_labels['TC'] * 100,\n",
        "              'ET': num_correct['ET'] / num_true_labels['ET'] * 100,\n",
        "              'WT': num_correct['WT'] / num_true_labels['WT'] * 100}\n",
        "\n",
        "    # f1 is actually the same as Dice Coefficient, or is it?\n",
        "    f1 = {'TC': 2 * precision['TC'] * recall['TC'] / (precision['TC'] + recall['TC']),\n",
        "          'ET': 2 * precision['ET'] * recall['ET'] / (precision['ET'] + recall['ET']),\n",
        "          'WT': 2 * precision['WT'] * recall['WT'] / (precision['WT'] + recall['WT'])}\n",
        "\n",
        "    items_size = preds.shape[0] * len(data_loader)\n",
        "\n",
        "    print(\n",
        "        f\" Accuracy (TC,ET,WT): \\n {num_correct['TC'] / num_pixels['TC'] * 100:.4f} , {num_correct['ET'] / num_pixels['ET'] * 100:.4f}, {num_correct['WT'] / num_pixels['WT'] * 100:.4f}\")\n",
        "    print(\n",
        "        f\"Dice Score (TC,ET,WT): \\n {dice_score['TC'] / items_size} , {dice_score['ET'] / items_size}, {dice_score['WT'] / items_size}\")\n",
        "    print(\n",
        "        f\"IoU Score (TC,ET,WT): \\n {iou_score['TC'] / items_size} , {iou_score['ET'] / items_size}, {iou_score['WT'] / items_size}\")\n",
        "\n",
        "    print(\n",
        "        f\" Precision (TC,ET,WT): \\n --> {precision['TC'] / items_size:.4f} , {precision['ET'] / items_size:.4f}, {precision['WT'] / items_size:.4f}\")\n",
        "    print(\n",
        "        f\" Recall (TC,ET,WT): \\n --> {recall['TC'] / items_size:.4f} , {recall['ET'] / items_size:.4f}, {recall['WT'] / items_size:.4f}\")\n",
        "    print(\n",
        "        f\" F1-score (TC,ET,WT): \\n --> {f1['TC'] / items_size:.4f} , {f1['ET'] / items_size:.4f}, {f1['WT'] / items_size:.4f}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "pdsiUnQzw3Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "FR_1y0-82ETP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Layers\n",
        "def default_norm_layer(planes, groups=16):\n",
        "    groups_ = min(groups, planes)\n",
        "    if planes % groups_ > 0:\n",
        "        divisor = 16\n",
        "        while planes % divisor > 0:\n",
        "            divisor /= 2\n",
        "        groups_ = int(planes // divisor)\n",
        "    return nn.GroupNorm(groups_, planes)\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type=\"group\"):\n",
        "    if \"group\" in norm_type:\n",
        "        try:\n",
        "            grp_nb = int(norm_type.replace(\"group\", \"\"))\n",
        "            return lambda planes: default_norm_layer(planes, groups=grp_nb)\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "            print('using default group number')\n",
        "            return default_norm_layer\n",
        "    elif norm_type == \"none\":\n",
        "        return None\n",
        "    else:\n",
        "        return lambda x: nn.InstanceNorm3d(x, affine=True)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1, bias=False):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=bias, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1, bias=True):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\n",
        "\n",
        "\n",
        "class ConvBnRelu(nn.Sequential):\n",
        "\n",
        "    def __init__(self, inplanes, planes, norm_layer=None, dilation=1, dropout=0):\n",
        "        if norm_layer is not None:\n",
        "            super(ConvBnRelu, self).__init__(\n",
        "                OrderedDict(\n",
        "                    [\n",
        "                        ('conv', conv3x3(inplanes, planes, dilation=dilation)),\n",
        "                        ('bn', norm_layer(planes)),\n",
        "                        ('relu', nn.ReLU(inplace=True)),\n",
        "                        ('dropout', nn.Dropout(p=dropout)),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            super(ConvBnRelu, self).__init__(\n",
        "                OrderedDict(\n",
        "                    [\n",
        "                        ('conv', conv3x3(inplanes, planes, dilation=dilation, bias=True)),\n",
        "                        ('relu', nn.ReLU(inplace=True)),\n",
        "                        ('dropout', nn.Dropout(p=dropout)),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "class UBlock(nn.Sequential):\n",
        "    \"\"\"Unet mainstream downblock.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inplanes, midplanes, outplanes, norm_layer, dilation=(1, 1), dropout=0):\n",
        "        super(UBlock, self).__init__(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    ('ConvBnRelu1', ConvBnRelu(inplanes, midplanes, norm_layer, dilation=dilation[0], dropout=dropout)),\n",
        "                    (\n",
        "                        'ConvBnRelu2',\n",
        "                        ConvBnRelu(midplanes, outplanes, norm_layer, dilation=dilation[1], dropout=dropout)),\n",
        "                ])\n",
        "        )\n",
        "\n",
        "\n",
        "class UBlockCbam(nn.Sequential):\n",
        "    def __init__(self, inplanes, midplanes, outplanes, norm_layer, dilation=(1, 1), dropout=0):\n",
        "        super(UBlockCbam, self).__init__(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    ('UBlock', UBlock(inplanes, midplanes, outplanes, norm_layer, dilation=dilation, dropout=dropout)),\n",
        "                    ('CBAM', CBAM(outplanes, norm_layer=norm_layer)),\n",
        "                ])\n",
        "        )\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "    #print(UBlock(4, 4))\n",
        "\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
        "                 norm_layer=None):\n",
        "        super(BasicConv, self).__init__()\n",
        "        bias = False\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=bias)\n",
        "        #self.bn = norm_layer(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        #x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "        )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = F.avg_pool3d(x, (x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                channel_att_raw = self.mlp(avg_pool)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = F.max_pool3d(x, (x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                channel_att_raw = self.mlp(max_pool)\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).unsqueeze(4).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n",
        "\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self, norm_layer=None):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size - 1) // 2, norm_layer=norm_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = torch.sigmoid(x_out)  # broadcasting\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=None, norm_layer=None):\n",
        "        super(CBAM, self).__init__()\n",
        "        if pool_types is None:\n",
        "            pool_types = ['avg', 'max']\n",
        "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
        "        self.SpatialGate = SpatialGate(norm_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        x_out = self.SpatialGate(x_out)\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "i6_tII4glkHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntermediateSequential(nn.Sequential):\n",
        "    def __init__(self, *args, return_intermediate=True):\n",
        "        super().__init__(*args)\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self, input):\n",
        "        if not self.return_intermediate:\n",
        "            return super().forward(input)\n",
        "\n",
        "        intermediate_outputs = {}\n",
        "        output = input\n",
        "        for name, module in self.named_children():\n",
        "            output = intermediate_outputs[name] = module(output)\n",
        "\n",
        "        return output, intermediate_outputs"
      ],
      "metadata": {
        "id": "VALW4WN3kBfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = heads\n",
        "        head_dim = dim // heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(dropout_rate)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x)\n",
        "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[0],\n",
        "            qkv[1],\n",
        "            qkv[2],\n",
        "        )  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(x) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x))\n",
        "\n",
        "\n",
        "class PreNormDrop(nn.Module):\n",
        "    def __init__(self, dim, dropout_rate, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fn(self.norm(x)))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads,\n",
        "        mlp_dim,\n",
        "        dropout_rate=0.1,\n",
        "        attn_dropout_rate=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(depth):\n",
        "            layers.extend(\n",
        "                [\n",
        "                    Residual(\n",
        "                        PreNormDrop(\n",
        "                            dim,\n",
        "                            dropout_rate,\n",
        "                            SelfAttention(dim, heads=heads, dropout_rate=attn_dropout_rate),\n",
        "                        )\n",
        "                    ),\n",
        "                    Residual(\n",
        "                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout_rate))\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "            # dim = dim / 2\n",
        "        self.net = IntermediateSequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "egb12ggmkGCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Positional Encoding\n",
        "class FixedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_length=512):\n",
        "        super(FixedPositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_length, embedding_dim)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embedding_dim, 2).float()\n",
        "            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, 512, 256)) #8x\n",
        "\n",
        "    def forward(self, x, position_ids=None):\n",
        "\n",
        "        position_embeddings = self.position_embeddings\n",
        "        return x + position_embeddings\n",
        "\n",
        "class LearnedPositionalEncoding_4(nn.Module):\n",
        "    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n",
        "        super(LearnedPositionalEncoding_4, self).__init__()\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, 2048, 512)) #8x\n",
        "\n",
        "    def forward(self, x, position_ids=None):\n",
        "\n",
        "        position_embeddings = self.position_embeddings\n",
        "        return x + position_embeddings"
      ],
      "metadata": {
        "id": "2x0dHExpk5mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "WzfZ8LzXk9KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogger:\n",
        "\n",
        "    def __init__(self, debug=True):\n",
        "        self.debug = debug\n",
        "\n",
        "    def enable_debug(self):\n",
        "        self.debug = True\n",
        "\n",
        "    def disable_debug(self):\n",
        "        self.debug = False\n",
        "\n",
        "    def log(self, message, condition=True):\n",
        "        if self.debug and condition:\n",
        "            print(message)\n",
        "\n",
        "\n",
        "logger = SimpleLogger(debug=True)\n",
        "\n",
        "\"\"\"\n",
        "class DoubleConv3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # 1 + (L - l + 2P)/s\n",
        "        self.conv = nn.Sequential(\n",
        "            # 1 + out - 3 + 2 = out\n",
        "            nn.Conv3d(in_channels, out_channels, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.conv(inputs)\n",
        "\"\"\"\n",
        "#3D UNET\n",
        "class Base3DUNet(BiTransformerUnet):\n",
        "    # the default dataset has 3 channels of data ->  T1CE, T2, FLAIR\n",
        "    # The output has background, NCR/NET, ED, ET\n",
        "    def __init__(self, in_channels=3, out_channels=4, features=[64, 128, 256, 512], up_sample=False):\n",
        "        super(BiTransformerUnet, self).__init__(\n",
        "            img_dim=img_dim,\n",
        "            patch_dim=patch_dim,\n",
        "            num_channels=num_channels,\n",
        "            embedding_dim_4=embedding_dim_4,\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attn_dropout_rate=attn_dropout_rate,\n",
        "            conv_patch_representation=conv_patch_representation,\n",
        "            positional_encoding_type=positional_encoding_type,\n",
        "        )\n",
        "        # 1 + (L - l + 2P)/s\n",
        "        # 1 + (L - 2)/2 = L\n",
        "        self.pooling = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Each Layer - number of filters , see UNet architecture\n",
        "        input_channels = in_channels\n",
        "\n",
        "        for feature in features:\n",
        "            self.downs.append(BiTransformerUnet(input_channels, feature))\n",
        "            input_channels = feature\n",
        "\n",
        "        for feature in reversed(features):\n",
        "            if up_sample:\n",
        "                self.ups.append(nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=True))\n",
        "                self.ups.append(BiTransformerUnet(feature * 3, feature))\n",
        "                # *3 because upsample does not change the number of channels\n",
        "            else:\n",
        "                self.ups.append(nn.ConvTranspose3d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "                self.ups.append(BiTransformerUnet(feature * 2, feature))\n",
        "\n",
        "        self.bottleneck = BiTransformerUnet(features[-1], features[-1] * 2)  # this connects downs to ups\n",
        "\n",
        "        self.output_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)  # last layer - feature compression\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        skips = []\n",
        "\n",
        "        x = inputs\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "            x = self.pooling(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):  # going up 2 steps, as each step has convTranspose and DoubleConv\n",
        "            x = self.ups[idx](x)  # up sampling w/ the convTranspose\n",
        "            skip_connection = skips.pop()  # give me the last skip I added, to add it first on the ups\n",
        "            x = torch.cat((skip_connection, x), dim=1)  # dim 0 is batch, dim 1 is the channels\n",
        "            x = self.ups[idx + 1](x)  # double conv\n",
        "\n",
        "        return self.output_conv(x)\n",
        "\n",
        "class BiTransformerUnet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_dim,\n",
        "        patch_dim,\n",
        "        num_channels,\n",
        "        embedding_dim_4,\n",
        "        embedding_dim,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        hidden_dim,\n",
        "        dropout_rate=0.0,\n",
        "        attn_dropout_rate=0.0,\n",
        "        conv_patch_representation=True,\n",
        "        positional_encoding_type=\"learned\",\n",
        "    ):\n",
        "        super(Attention3UNet, self).__init__()\n",
        "\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        assert img_dim % patch_dim == 0\n",
        "\n",
        "        self.img_dim = img_dim\n",
        "        self.embedding_dim_4 = embedding_dim_4\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_dim = patch_dim\n",
        "        self.num_channels = num_channels\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn_dropout_rate = attn_dropout_rate\n",
        "        self.conv_patch_representation = conv_patch_representation\n",
        "\n",
        "        self.num_patches = int((img_dim // patch_dim) ** 3)\n",
        "        self.seq_length = self.num_patches\n",
        "        self.flatten_dim = 128 * num_channels\n",
        "\n",
        "        self.linear_encoding = nn.Linear(self.flatten_dim, self.embedding_dim)\n",
        "        if positional_encoding_type == \"learned\":\n",
        "            self.position_encoding = LearnedPositionalEncoding(\n",
        "                self.seq_length, self.embedding_dim, self.seq_length\n",
        "            )\n",
        "            self.position_encoding_4 = LearnedPositionalEncoding_4(\n",
        "                self.seq_length, self.embedding_dim_4, self.seq_length\n",
        "            )\n",
        "        elif positional_encoding_type == \"fixed\":\n",
        "            self.position_encoding = FixedPositionalEncoding(\n",
        "                self.embedding_dim,\n",
        "            )\n",
        "            self.position_encoding_4 = FixedPositionalEncoding(\n",
        "                self.embedding_dim_4,\n",
        "            )\n",
        "\n",
        "        self.pe_dropout = nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "        self.transformer = TransformerModel(\n",
        "            embedding_dim,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "\n",
        "            self.dropout_rate,\n",
        "            self.attn_dropout_rate,\n",
        "        )\n",
        "\n",
        "        self.transformer_4 = TransformerModel(\n",
        "            embedding_dim_4,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "\n",
        "            self.dropout_rate,\n",
        "            self.attn_dropout_rate,\n",
        "        )\n",
        "\n",
        "        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n",
        "        self.pre_head_ln_4 = nn.LayerNorm(embedding_dim_4)\n",
        "\n",
        "        if self.conv_patch_representation:\n",
        "\n",
        "            self.conv_x = nn.Conv3d(\n",
        "                256,\n",
        "                self.embedding_dim,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            )\n",
        "\n",
        "        if self.conv_patch_representation:\n",
        "\n",
        "            self.conv_x_4 = nn.Conv3d(\n",
        "                128,\n",
        "                self.embedding_dim,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            )\n",
        "\n",
        "        #Encoder\n",
        "        self.Unet = Base3DUNet(inplanes = 16, num_classes = 4 , width = 16)\n",
        "        self.bn = nn.BatchNorm3d(256)\n",
        "        self.bn_4 = nn.BatchNorm3d(128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        if self.conv_patch_representation:\n",
        "            # combine embedding with conv patch distribution\n",
        "            x1_1, x2_1, x3_1, x4_1, x = self.Unet(x)\n",
        "\n",
        "            x4_t = self.bn_4(x4_1) #\n",
        "            x4_t= self.relu(x4_t) #\n",
        "            x4_t= self.conv_x_4(x4_t) #\n",
        "            x4_t = x4_t.permute(0, 2, 3, 4, 1).contiguous() #\n",
        "            x4_t= x4_t.view(x4_t.size(0), -1, self.embedding_dim_4) #\n",
        "\n",
        "            x = self.bn(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.conv_x(x)\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size(0), -1, self.embedding_dim)\n",
        "\n",
        "        else:\n",
        "            x = self.Unet(x)\n",
        "            x = self.bn(x)\n",
        "            x = self.relu(x)\n",
        "            x = (\n",
        "                x.unfold(2, 2, 2)\n",
        "                .unfold(3, 2, 2)\n",
        "                .unfold(4, 2, 2)\n",
        "                .contiguous()\n",
        "            )\n",
        "            x = x.view(x.size(0), x.size(1), -1, 8)\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size(0), -1, self.flatten_dim)\n",
        "            x = self.linear_encoding(x)\n",
        "\n",
        "        x4_t = self.position_encoding_4(x4_t) #\n",
        "        x4_t = self.pe_dropout(x4_t) #\n",
        "\n",
        "        x = self.position_encoding(x)\n",
        "        x = self.pe_dropout(x)\n",
        "\n",
        "        # apply transformer\n",
        "        x4_t, intmd_x4_t = self.transformer_4(x4_t) #\n",
        "        x4_t = self.pre_head_ln_4(x4_t) #\n",
        "\n",
        "        x, intmd_x = self.transformer(x)\n",
        "        x = self.pre_head_ln(x)\n",
        "\n",
        "        return x1_1, x2_1, x3_1, x4_1, x, intmd_x, x4_t, intmd_x4_t\n",
        "\n",
        "    def decode(self, x):\n",
        "        raise NotImplementedError(\"Should be implemented in child class!!\")\n",
        "\n",
        "    def forward(self, x, auxillary_output_layers=[1, 2, 3, 4], auxillary_output_layers_4=[1, 2, 3, 4]):\n",
        "\n",
        "        x1_1, x2_1, x3_1, x4_1, encoder_output, intmd_encoder_outputs, encoder_output_4, intmd_encoder_outputs_4 = self.encode(x)\n",
        "\n",
        "        decoder_output = self.decode(\n",
        "            x1_1, x2_1, x3_1, x4_1, encoder_output, intmd_encoder_outputs, encoder_output_4, intmd_encoder_outputs_4, auxillary_output_layers, auxillary_output_layers_4\n",
        "        )\n",
        "\n",
        "        if auxillary_output_layers is not None:\n",
        "            auxillary_outputs = {}\n",
        "            for i in auxillary_output_layers:\n",
        "                val = str(2 * i - 1)\n",
        "                _key = 'Z' + str(i)\n",
        "                auxillary_outputs[_key] = intmd_encoder_outputs[val]\n",
        "\n",
        "        if auxillary_output_layers_4 is not None:\n",
        "            auxillary_outputs_4 = {}\n",
        "            for i in auxillary_output_layers_4:\n",
        "                val_4 = str(2 * i - 1)\n",
        "                _key_4 = 'Z' + str(i)\n",
        "                auxillary_outputs_4[_key_4] = intmd_encoder_outputs_4[val_4]\n",
        "\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "    def _get_padding(self, padding_type, kernel_size):\n",
        "        assert padding_type in ['SAME', 'VALID']\n",
        "        if padding_type == 'SAME':\n",
        "            _list = [(k - 1) // 2 for k in kernel_size]\n",
        "            return tuple(_list)\n",
        "        return tuple(0 for _ in kernel_size)\n",
        "\n",
        "    def _reshape_output(self, x):\n",
        "        x = x.view(\n",
        "            x.size(0),\n",
        "            int(self.img_dim / self.patch_dim),\n",
        "            int(self.img_dim / self.patch_dim),\n",
        "            int(self.img_dim / self.patch_dim),\n",
        "            self.embedding_dim,\n",
        "        )\n",
        "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _reshape_output_4(self, x):\n",
        "        x = x.view(\n",
        "            x.size(0),\n",
        "            int(self.img_dim*2 / self.patch_dim),\n",
        "            int(self.img_dim*2 / self.patch_dim),\n",
        "            int(self.img_dim *2/ self.patch_dim),\n",
        "            self.embedding_dim,\n",
        "        )\n",
        "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # Xs come from the encoder and Gs come from previous lower layer and a point-wise conv is applied to both and\n",
        "    # their sum will be calculated and fed into ReLU Then another point-wise conv (Psi) is applied with a sigmoid after\n",
        "    # that, which is supposed to be the probability map of each data point (pixel) hence it is multiplied with the X\n",
        "\n",
        "    # Why we don't multiply it with G?\n",
        "    # My Intuition: because more information will be available in X and its size is closer to the current module\n",
        "    # if G is used then we need rescaling or padding, which makes it useless, on the other hand we already have the\n",
        "    # effect of both\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0), nn.BatchNorm3d(F_int))\n",
        "        self.W_x = nn.Sequential(nn.Conv3d(F_l, F_int, kernel_size=1, stride=1, padding=0), nn.BatchNorm3d(F_int))\n",
        "        self.psi = nn.Sequential(nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0), nn.BatchNorm3d(1))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g = self.W_g(g)\n",
        "        x_o = self.W_x(x)\n",
        "        relu_sum = self.relu(g + x_o)\n",
        "        psi = self.psi(relu_sum)\n",
        "        psi = torch.sigmoid(psi)\n",
        "        return x * psi\n",
        "\n",
        "\n",
        "class Attention3UNet(Base3DUNet):  # Generalization of 2D attention UNet\n",
        "    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512], up_sample=True):\n",
        "        super().__init__(in_channels, out_channels, features, up_sample=up_sample)\n",
        "        self.attention = nn.ModuleList()\n",
        "\n",
        "        for feature in reversed(features):\n",
        "            if up_sample:\n",
        "                self.attention.append(AttentionBlock(feature * 2, feature, feature))\n",
        "            else:\n",
        "                self.attention.append(AttentionBlock(feature, feature, feature))\n",
        "                # when convT is used, then the G is normal and the same as skip\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        skips = []\n",
        "\n",
        "        x = inputs\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "            x = self.pooling(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # same as original Unet till here\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):  # going up 2 steps, as each step has convTranspose and DoubleConv\n",
        "            x = self.ups[idx](x)  # up sampling w/ the convTranspose or Upsampler\n",
        "\n",
        "            # ---> DIFF START: the skip_connection acquired is the X_l in attention UNet paper, and x is G\n",
        "            skip_connection = skips.pop()  # give me the last skip I added, to add it first on the ups\n",
        "            skip_connection = self.attention[idx // 2](x, skip_connection)  # (G,X) = (x, skip_connection) in here\n",
        "            # <---- end of difference between attention and base 3D UNet\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)  # dim 0 is batch, dim 1 is the channels\n",
        "            x = self.ups[idx + 1](x)  # double conv\n",
        "\n",
        "        return self.output_conv(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "kURDrc8xzfI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DSC = 2 * |A intersect B| / (|A| + |B|)\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Calculate dice loss.\"\"\"\n",
        "\n",
        "    def __init__(self, eps: float = 1e-9):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self,\n",
        "                logits: torch.Tensor,\n",
        "                targets: torch.Tensor) -> torch.Tensor:\n",
        "        num = targets.size(0)\n",
        "        probability = torch.sigmoid(logits)\n",
        "        probability = probability.view(num, -1)\n",
        "        targets = targets.view(num, -1)\n",
        "        assert (probability.shape == targets.shape)\n",
        "\n",
        "        intersection = 2.0 * (probability * targets).sum()\n",
        "        union = probability.sum() + targets.sum()\n",
        "        dice_score = (intersection + self.eps) / union\n",
        "        # print(\"intersection\", intersection, union, dice_score)\n",
        "        return 1.0 - dice_score\n",
        "\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BCEDiceLoss, self).__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.dice = DiceLoss()\n",
        "\n",
        "    def forward(self,\n",
        "                logits: torch.Tensor,\n",
        "                targets: torch.Tensor) -> torch.Tensor:\n",
        "        assert (logits.shape == targets.shape)\n",
        "        dice_loss = self.dice(logits, targets)\n",
        "        bce_loss = self.bce(logits, targets.float())\n",
        "\n",
        "        return bce_loss + dice_loss"
      ],
      "metadata": {
        "id": "pEULHk-n2PoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 100\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "5-TtTMAI2ny3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.0001"
      ],
      "metadata": {
        "id": "9TjMeOEi2wwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = get_dl(get_train_ds(), BATCH_SIZE,nw=2)\n",
        "val_dl = get_dl(get_val_ds(), BATCH_SIZE, nw=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCSzqM23EDaa",
        "outputId": "fde86665-80b5-4aa2-d6e5-a058518b04c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images: 221, masks: 221 \n",
            "images: 148, masks: 148 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Attention3UNet(3, 3, features=[64, 128, 256, 512], up_sample=False).to(DEVICE)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "4Tr6pq6CJWyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "2d66da09-ff04-4f6f-cd06-3a000a2ef3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-48e2dcaaeddb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention3UNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-39ad114e91d6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, features, up_sample)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttention3UNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBase3DUNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Generalization of 2D attention UNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mup_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-39ad114e91d6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, features, up_sample)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# The output has background, NCR/NET, ED, ET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         super(BiTransformerUnet, self).__init__(\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mimg_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpatch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total parameters = {sum(p.numel() for p in model.parameters())}\") # 5m more params than base3DUnet\n",
        "print(f\"total learnable parameters = {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suKSDRxDTg8T",
        "outputId": "fddc05c4-4984-4354-fa55-e8d6a7201847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total parameters = 91005199\n",
            "total learnable parameters = 91005199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "# loss = torch.nn.BCEWithLogitsLoss # cannot be used because there's a lot of imbalance anyway, so it is better to combine it with dice\n",
        "loss = BCEDiceLoss()\n",
        "# loss = DiceLoss()"
      ],
      "metadata": {
        "id": "eO8nbAGKTiRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs=EPOCHS, training_loader=train_dl, loss_fn=loss, device=DEVICE, optimizer=opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9WAotvqTlVR",
        "outputId": "6449db64-8f4d-45d8-955f-3149c4d10608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "At epoch [1/100]: 100%|| 56/56 [02:42<00:00,  2.91s/it, loss=1.4]\n",
            "At epoch [2/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=1.27]\n",
            "At epoch [3/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=1.26]\n",
            "At epoch [4/100]: 100%|| 56/56 [01:42<00:00,  1.82s/it, loss=1.27]\n",
            "At epoch [5/100]: 100%|| 56/56 [01:42<00:00,  1.84s/it, loss=1.21]\n",
            "At epoch [6/100]: 100%|| 56/56 [01:41<00:00,  1.82s/it, loss=1.22]\n",
            "At epoch [7/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=1.1]\n",
            "At epoch [8/100]: 100%|| 56/56 [01:41<00:00,  1.82s/it, loss=1.09]\n",
            "At epoch [9/100]: 100%|| 56/56 [01:42<00:00,  1.84s/it, loss=0.973]\n",
            "At epoch [10/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=0.985]\n",
            "At epoch [11/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=0.797]\n",
            "At epoch [12/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=0.985]\n",
            "At epoch [13/100]: 100%|| 56/56 [01:42<00:00,  1.84s/it, loss=0.905]\n",
            "At epoch [14/100]: 100%|| 56/56 [01:42<00:00,  1.82s/it, loss=0.902]\n",
            "At epoch [15/100]: 100%|| 56/56 [01:41<00:00,  1.82s/it, loss=0.692]\n",
            "At epoch [16/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=0.655]\n",
            "At epoch [17/100]: 100%|| 56/56 [01:42<00:00,  1.83s/it, loss=0.76]\n",
            "At epoch [18/100]: 100%|| 56/56 [01:42<00:00,  1.84s/it, loss=0.888]\n",
            "At epoch [19/100]:  20%|        | 11/56 [00:23<01:22,  1.83s/it, loss=0.596]"
          ]
        }
      ]
    }
  ]
}